{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pendulum\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_night(time_list):\n",
    "    '''\n",
    "    发微博的时间序列转化为24个时间段的统计\n",
    "    :param in_name:\n",
    "    :param out_name:\n",
    "    :return: \n",
    "    '''\n",
    "    cnt = [0] * 24\n",
    "    for t in time_list:\n",
    "        hour = t.hour\n",
    "        # print(hour)\n",
    "        # 分为四个时间段\n",
    "        # cnt[int(int(hour) / 6)] += 1\n",
    "        # 分为24小时\n",
    "        cnt[int(hour)] += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def weeks(time_list):\n",
    "    '''\n",
    "    发微博的时间序列转化为7个时间段的统计\n",
    "    :param in_name:\n",
    "    :param out_name:\n",
    "    :return:\n",
    "    '''\n",
    "    cnt = [0] * 7\n",
    "    for t in time_list:\n",
    "        weekday = t.weekday()\n",
    "        cnt[int(weekday)] += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def time_interval(time_list):\n",
    "    interval = []\n",
    "    for i in np.arange(1, len(time_list)):\n",
    "        t2 = time_list[i-1]\n",
    "        t1 = time_list[i]\n",
    "        _inter = abs((t2 - t1).total_seconds())\n",
    "        if _inter < 86400 * 7:\n",
    "            interval.append(_inter / 3600)\n",
    "    # return interval\n",
    "    interval = np.array(interval)\n",
    "    # print(interval)\n",
    "    return interval\n",
    "\n",
    "\n",
    "def life_length(time_list):\n",
    "    return int((max(time_list) - min(time_list)).total_seconds() / 3600 / 24) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_features = []\n",
    "\n",
    "for in_name in Path(in_dir).rglob(\"*.json\"):\n",
    "    # print(in_name)\n",
    "    d = json.load(open(in_name))\n",
    "    user = d[\"user\"]\n",
    "    u = {\n",
    "        \"uid\": user[\"id\"],\n",
    "        \"gender\": 1 if user[\"gender\"]==\"男\" else 0,\n",
    "        \"have_talent\": int(user[\"talent\"] != \"\"),\n",
    "        \"have_education\": int(user[\"education\"] != \"\"),\n",
    "        \"have_work\": int(user[\"work\"] != \"\"),\n",
    "        \"NT\": np.log(user[\"weibo_num\"] + 1),\n",
    "        \"NFEE\": np.log(user[\"following\"] + 1),\n",
    "        \"NEER\": np.log(user[\"followers\"] + 1),\n",
    "        \"NT/NFEE\": np.log((user[\"weibo_num\"] + 1) / (user[\"following\"] + 1)),\n",
    "        \"NT/NFER\": np.log((user[\"weibo_num\"] + 1) / (user[\"followers\"] + 1)),\n",
    "        \"NEEE/NFER\": np.log((user[\"following\"] + 1) / (user[\"followers\"] + 1)),\n",
    "        \"len_desc\": len(user[\"description\"]),\n",
    "    }\n",
    "    weibos = d[\"weibo\"]\n",
    "    # 所有微博\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos]\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"h_{i}\"] = _d\n",
    "    u[\"h_max\"] = d_features.max()\n",
    "    u[\"h_argmax\"] = d_features.argmax()\n",
    "    u[\"h_std\"] = d_features.std()\n",
    "    \n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"w_{i}\"] = _d\n",
    "    u[\"w_max\"] = w_features.max()\n",
    "    u[\"w_argmax\"] = w_features.argmax()\n",
    "    u[\"w_std\"] = w_features.std()\n",
    "\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"interval_mean\"] = 0\n",
    "        u[\"interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"interval_mean\"] = interval.mean()\n",
    "        u[\"interval_std\"] = interval.std()\n",
    "\n",
    "    u[\"life_length\"] = np.log(life_length(time_list) + 1)\n",
    "    u[\"ave_d_num\"] = user[\"weibo_num\"] / u[\"life_length\"]\n",
    "\n",
    "    # 转发微博\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos if not w[\"original\"]]\n",
    "    u[\"ret_prop\"] = len(time_list) / (user[\"weibo_num\"] + 1)\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"ret_h_{i}\"] = _d\n",
    "    u[\"ret_h_max\"] = d_features.max()\n",
    "    u[\"ret_h_argmax\"] = d_features.argmax()\n",
    "    u[\"ret_h_std\"] = d_features.std()\n",
    "     \n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"ret_w_{i}\"] = _d\n",
    "    u[\"ret_w_max\"] = w_features.max()\n",
    "    u[\"ret_w_argmax\"] = w_features.argmax()\n",
    "    u[\"ret_w_std\"] = w_features.std()\n",
    "\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"ret_interval_mean\"] = 0\n",
    "        u[\"ret_interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"ret_interval_mean\"] = interval.mean()\n",
    "        u[\"ret_interval_std\"] = interval.std()\n",
    "        \n",
    "    # 提及（@）微博\n",
    "    time_list = [pendulum.parse(w[\"publish_time\"]) for w in weibos if \"@\" in w[\"content\"]]\n",
    "    u[\"men_prop\"] = len(time_list) / (user[\"weibo_num\"] + 1)\n",
    "    d_features = np.array(day_night(time_list))\n",
    "    for i, _d in enumerate(d_features):\n",
    "        u[f\"men_h_{i}\"] = _d\n",
    "    u[\"men_h_max\"] = d_features.max()\n",
    "    u[\"men_h_argmax\"] = d_features.argmax()\n",
    "    u[\"men_h_std\"] = d_features.std()\n",
    "    \n",
    "    w_features = np.array(weeks(time_list))\n",
    "    for i, _d in enumerate(w_features):\n",
    "        u[f\"men_w_{i}\"] = _d\n",
    "    u[\"men_w_max\"] = w_features.max()\n",
    "    u[\"men_w_argmax\"] = w_features.argmax()\n",
    "    u[\"men_w_std\"] = w_features.std()\n",
    "\n",
    "    interval = time_interval(time_list)\n",
    "    if len(interval) < 1:\n",
    "        u[\"men_interval_mean\"] = 0\n",
    "        u[\"men_interval_std\"] = 0\n",
    "    else:\n",
    "        u[\"men_interval_mean\"] = interval.mean()\n",
    "        u[\"men_interval_std\"] = interval.std()\n",
    "        \n",
    "    # 文本特征\n",
    "    # 另外的文件\n",
    "    users_features.append(u)\n",
    "    # print(user)\n",
    "    \n",
    "# len(users_features)\n",
    "\n",
    "df = pd.DataFrame(users_features).set_index(\"uid\")\n",
    "df.to_csv(\"csv\", float_format=\"%.4f\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分析\n",
    "from vocab import vocab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_list = vocab.community_keywords \\\n",
    "            + vocab.financial_keywords \\\n",
    "            + vocab.popularity_keywords \\\n",
    "            + vocab.image_keywords \\\n",
    "            + vocab.health_keywords \\\n",
    "            + vocab.affiliation_keywords \\\n",
    "            + vocab.selfacceptance_keywords\n",
    "word_list = list(set(word_list))\n",
    "word_list.sort(key=lambda x: len(x), reverse=True)\n",
    "# print(word_list)\n",
    "print(len(word_list))\n",
    "with open(\"vocab/word_list.txt\", \"w\") as f:\n",
    "    for w in word_list:\n",
    "        f.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=10)\n",
    "print(len(users_corpus))\n",
    "tfidf = vectorizer.fit_transform(users_corpus)\n",
    "X = tfidf.toarray()\n",
    "# print(X.sum(axis=0))\n",
    "print(X.shape)\n",
    "\n",
    "text_df = pd.DataFrame(X, columns=[\"wc\"+str(i+1) for i in range(X.shape[1])])\n",
    "text_df = text_df.loc[:, (text_df != 0).any(axis=0)]\n",
    "text_df.index = [in_name.name[:-5] for in_name in Path(in_dir).rglob(\"*.json\")]\n",
    "text_df.index.name = \"uid\"\n",
    "text_df.to_csv(\"csv\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10)\n",
    "print(vectorizer.vocabulary_())\n",
    "print(len(users_corpus))\n",
    "tfidf = vectorizer.fit_transform(users_corpus)\n",
    "X = tfidf.toarray()\n",
    "print(X.shape)\n",
    "\n",
    "text_df = pd.DataFrame(X, columns=[\"tfidf\"+str(i+1) for i in range(X.shape[1])])\n",
    "text_df = text_df.loc[:, (text_df != 0).any(axis=0)]\n",
    "text_df.index = [in_name.name[:-5] for in_name in Path(in_dir).rglob(\"*.json\")]\n",
    "text_df.index.name = \"uid\"\n",
    "text_df.to_csv(\"csv\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入问卷数据\n",
    "survey_data = pd.read_excel(\"xlsx\")\n",
    "survey_data[\"userID_num\"].astype(\"str\")\n",
    "survey_data = survey_data.set_index(\"userID_num\")\n",
    "survey_data.index.name = \"uid\"\n",
    "survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_data[[c for c in survey_data.columns.to_list() if c.startswith(\"DV_\") or c.startswith(\"e_\")]]\n",
    "survey_data = survey_data[[c for c in survey_data.columns.to_list() if \"@\" not in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for index, c in survey_data.iteritems():\n",
    "    if not index.startswith(\"DV\"):\n",
    "        continue\n",
    "    print(index)\n",
    "    c = [[_c] for _c in c]\n",
    "    \n",
    "    y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(c)\n",
    "    print(Counter(y_pred))\n",
    "    survey_data.loc[:, \"C2\" + index] = y_pred\n",
    "    \n",
    "    y_pred = KMeans(n_clusters=3, random_state=42).fit_predict(c)\n",
    "    print(Counter(y_pred))\n",
    "    survey_data.loc[:, \"C3\" + index] = y_pred\n",
    "    \n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data.to_csv(\"csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa78a3bd2db5949dc15dc50c0d566b6d40eb6f5f2fccd02bd2862e501e9d9051"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
